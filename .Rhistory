# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda",sparse = T)
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
#Float32 is slightly faster
Xgpu <-     gpu.matrix(X, dtype = "float32")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda",sparse = T)
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
crossprod(X,2*y-1)
X
dim(X %*% beta)
X %*% beta
beta
beta <- solve(crossprod(X),crossprod(X,2*y-1)) # Returns double even with float inputs. Fix!
dim(X %*% beta)
train_cglr <- function(X,y,beta = NULL, lambda = 0, iterations = 10, tol = 1e-6) {
tX <- t(X)
if (is.null(beta))
beta <- solve(crossprod(X),crossprod(X,2*y-1)) # Returns double even with float inputs. Fix!
p <- sigmoid(X %*% beta)
a <- p*(1-p)
# g <- gr(beta,X,y)
g <- tX %*% (p - y)
u <- g
u_old <- u
for (iter in 1:iterations) {
if (iter == 1) {
uhu <- (sum(u*(tX %*% (a * (X %*% u)))) + lambda * sum(u*u))
beta <- beta-sum(g*u)/uhu * u
} else{
p <- sigmoid(X %*% beta)
beta_old <- beta
g_old <- g
a <- p*(1-p)
g <- tX %*% (p - y)
# g <- t(crossprod(p-y,X)) # Avoid the transposition of a sparse matrix. TODO: remove warnings
k <- g - g_old
beta_coef <- sum(g * k)/sum(u_old*k) # beta Hestenes-Stiefel. Other options out there
u <- g - u_old * beta_coef
u_old <- u
uhu <- sum((X %*% u)^2*a) + lambda * sum(u*u)
beta <- beta_old - sum(g*u)/uhu * u
if(sum(g*g)< tol)
break
}
}
print(iter)
return(beta)
}
profile(train_cglr)
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda",sparse = T)
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cuda")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float64",device="cuda")
system.time((fastLR(X,y)$coefficients))
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
sum(Xgpu@gm)
sum(Xgpu@gm)
GPUmatrix:::sum(Xgpu@gm)
GPUmatrix::sum(Xgpu@gm)
GPUmatrix::sum
library(GPUmatrix)
View(sum())
library(GPUmatrix)
library(Matrix)
library(matrixStats)
A <- Matrix(c(1,3,-1,2,8),5,1)
AG <- gpu.matrix(A, device = "cuda")
sum(AG)
sum(AG)
GPUmatrix::min(sum(AG))
View("sum")
sum
base::sum
show(sum)
showMethods(sum)
library(GPUmatrix)
library(GPUmatrix)
library(GPUmatrix)
# Efficient implementation
# Define the logistic function
sigmoid <- function(x) {
#.5 * (tanh(.5*x)+1)
# or
1/(1+exp(-x))
}
train_cglr <- function(X,y,beta = NULL, lambda = 0, iterations = 10, tol = 1e-6) {
tX <- t(X)
if (is.null(beta))
beta <- solve(crossprod(X),crossprod(X,2*y-1)) # Returns double even with float inputs. Fix!
p <- sigmoid(X %*% beta)
a <- p*(1-p)
# g <- gr(beta,X,y)
g <- tX %*% (p - y)
u <- g
u_old <- u
for (iter in 1:iterations) {
if (iter == 1) {
uhu <- (sum2(u*(tX %*% (a * (X %*% u)))) + lambda * sum2(u*u))
beta <- beta-sum2(g*u)/uhu * u
} else{
p <- sigmoid(X %*% beta)
beta_old <- beta
g_old <- g
a <- p*(1-p)
g <- tX %*% (p - y)
# g <- t(crossprod(p-y,X)) # Avoid the transposition of a sparse matrix. TODO: remove warnings
k <- g - g_old
beta_coef <- sum2(g * k)/sum2(u_old*k) # beta Hestenes-Stiefel. Other options out there
u <- g - u_old * beta_coef
u_old <- u
uhu <- sum2((X %*% u)^2*a) + lambda * sum2(u*u)
beta <- beta_old - sum2(g*u)/uhu * u
if(sum2(g*g)< tol)
break
}
}
print(iter)
return(beta)
}
n <- 30000
p <- 3000
X <- cbind(1, matrix(rnorm(n * p), n, p))*.1
beta_true <- rnorm(p+1)
linear_preds_true <- X %*% beta_true
y <- rbinom(n, 1, sigmoid(linear_preds_true))
Xgpu <-     gpu.matrix(X, dtype = "float32", device = "cpu")
ygpu <-     gpu.matrix(y, dtype = "float32", device = "cpu")
beta_true
betainit
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float32",device="cpu")
system.time((fastLR(X,y)$coefficients))
library(RcppNumerical)
library(GPUmatrix)
library(torch)
library(tensorflow)
library(profvis)
library(Matrix)
system.time((fastLR(X,y)$coefficients))
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
GPUmatrix::sum2(Xgpu)
library(GPUmatrix)
# Example usage:
set.seed(1234)
n <- 30000
p <- 3000
X <- cbind(1, matrix(rnorm(n * p), n, p))*.1
beta_true <- rnorm(p+1)
linear_preds_true <- X %*% beta_true
y <- rbinom(n, 1, sigmoid(linear_preds_true))
Xgpu <-     gpu.matrix(X, dtype = "float32", device = "cpu")
ygpu <-     gpu.matrix(y, dtype = "float32", device = "cpu")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float32",device="cpu")
system.time((fastLR(X,y)$coefficients))
sum2(Xgpu)
GPUmatrix::sum2(Xgpu)
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# Efficient implementation
# Define the logistic function
sigmoid <- function(x) {
#.5 * (tanh(.5*x)+1)
# or
1/(1+exp(-x))
}
train_cglr <- function(X,y,beta = NULL, lambda = 0, iterations = 10, tol = 1e-6) {
tX <- t(X)
if (is.null(beta))
beta <- solve(crossprod(X),crossprod(X,2*y-1)) # Returns double even with float inputs. Fix!
p <- sigmoid(X %*% beta)
a <- p*(1-p)
# g <- gr(beta,X,y)
g <- tX %*% (p - y)
u <- g
u_old <- u
for (iter in 1:iterations) {
if (iter == 1) {
uhu <- (GPUmatrix::sum2(u*(tX %*% (a * (X %*% u)))) + lambda * GPUmatrix::sum2(u*u))
beta <- beta-GPUmatrix::sum2(g*u)/uhu * u
} else{
p <- sigmoid(X %*% beta)
beta_old <- beta
g_old <- g
a <- p*(1-p)
g <- tX %*% (p - y)
# g <- t(crossprod(p-y,X)) # Avoid the transposition of a sparse matrix. TODO: remove warnings
k <- g - g_old
beta_coef <- GPUmatrix::sum2(g * k)/GPUmatrix::sum2(u_old*k) # beta Hestenes-Stiefel. Other options out there
u <- g - u_old * beta_coef
u_old <- u
uhu <- GPUmatrix::sum2((X %*% u)^2*a) + lambda * GPUmatrix::sum2(u*u)
beta <- beta_old - GPUmatrix::sum2(g*u)/uhu * u
if(GPUmatrix::sum2(g*g)< tol)
break
}
}
print(iter)
return(beta)
}
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
library(GPUmatrix)
Xgpu <-     gpu.matrix(X, dtype = "float32", device = "cpu")
ygpu <-     gpu.matrix(y, dtype = "float32", device = "cpu")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float32",device="cpu")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cpu")
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cpu")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda")
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cuda")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float32",device="cuda")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda")
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cuda")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float64",device="cuda")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda")
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cuda")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float64",device="cuda")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
system.time((fastLR(X,y)$coefficients))
#Float32 is slightly faster
dtype(Xgpu) <- "float32"
dtype(ygpu) <- "float32"
dtype(betainit) <- "float32"
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
system.time((fastLR(X,y)$coefficients))
n <- 30000
p <- 300
X <- cbind(1, matrix(rnorm(n * p), n, p))*.1
beta_true <- rnorm(p+1)
linear_preds_true <- X %*% beta_true
y <- rbinom(n, 1, sigmoid(linear_preds_true))
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cuda")
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cuda")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float64",device="cuda")
system.time((fastLR(X,y)$coefficients))
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
#Float32 is slightly faster
dtype(Xgpu) <- "float32"
dtype(ygpu) <- "float32"
dtype(betainit) <- "float32"
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
# Efficient implementation
# Define the logistic function
sigmoid <- function(x) {
#.5 * (tanh(.5*x)+1)
# or
1/(1+exp(-x))
}
train_cglr <- function(X,y,beta = NULL, lambda = 0, iterations = 10, tol = 1e-6) {
tX <- t(X)
if (is.null(beta))
beta <- solve(crossprod(X),crossprod(X,2*y-1)) # Returns double even with float inputs. Fix!
p <- sigmoid(X %*% beta)
a <- p*(1-p)
# g <- gr(beta,X,y)
g <- tX %*% (p - y)
u <- g
u_old <- u
for (iter in 1:iterations) {
if (iter == 1) {
uhu <- (sum(u*(tX %*% (a * (X %*% u)))) + lambda * sum(u*u))
beta <- beta-sum(g*u)/uhu * u
} else{
p <- sigmoid(X %*% beta)
beta_old <- beta
g_old <- g
a <- p*(1-p)
g <- tX %*% (p - y)
# g <- t(crossprod(p-y,X)) # Avoid the transposition of a sparse matrix. TODO: remove warnings
k <- g - g_old
beta_coef <- sum(g * k)/sum(u_old*k) # beta Hestenes-Stiefel. Other options out there
u <- g - u_old * beta_coef
u_old <- u
uhu <- sum((X %*% u)^2*a) + lambda * sum(u*u)
beta <- beta_old - sum(g*u)/uhu * u
if(sum(g*g)< tol)
break
}
}
print(iter)
return(beta)
}
Xgpu
Xgpu@type
dtype(Xgpu)
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
sum(Xgpu)
min(Xgpu)
library(GPUmatrix)
gpu.matrix(nrow = 1, ncol = 1)
matrix(nrow = 1, ncol = 1)
matrix(nrow = 4, ncol = 2)
torch::torch_tensor(device =torch_float64() ,dtype = "cuda")
torch::torch_tensor(matrix(nrow = 4, ncol = 2),device =torch_float64() ,dtype = "cuda")
torch::torch_tensor(matrix(nrow = 4, ncol = 2),device =torch_float64() ,dtype = "cpu")
torch::torch_tensor(matrix(nrow = 4, ncol = 2),device =torch_float64() ,dtype = torch_device("cuda"))
torch::torch_tensor(matrix(nrow = 4, ncol = 2),device =torch_float64() ,dtype = torch_device("cuda"))
torch_float64()
device_torch <- torch::torch_device(type = "cuda")
dtype <- castDtype_torch("float64")
dtype <- GPUmatrix:::castDtype_torch("float64")
torch::torch_tensor(matrix(nrow = 4, ncol = 2),device =device_torch ,dtype = dtype)
matrix(nrow = 4, ncol = 2)
torch::torch_tensor(matrix(nrow = 4, ncol = 2),device =device_torch ,dtype = dtype)
torch::torch_tensor(NULL,device =device_torch ,dtype = dtype)
torchNUll <- torch::torch_tensor(rep(na,nrow*ncol),device =device_torch ,dtype = dtype)
torchNUll <- torch::torch_tensor(rep(NULL,nrow*ncol),device =device_torch ,dtype = dtype)
torchNUll <- torch::torch_tensor(rep(NULL,4*4),device =device_torch ,dtype = dtype)
torchNUll
torchNUll <- torch::torch_tensor(matrix(NULL,4,4),device =device_torch ,dtype = dtype)
matrix(NULL,4,4)
torchNUll <- torch::torch_tensor(matrix(rep(NULL,4*4),4,4),device =device_torch ,dtype = dtype)
rep(NULL,4*4)
torchNUll <- torch::torch_tensor(matrix(rep(NA,4*4),4,4),device =device_torch ,dtype = dtype)
torchNUll
torch_empty(c(2,3))
data<-NA
class(data)
library(GPUmatrix)
gpu.matrix(nrow = 2,ncol = 4,dtype = "float64", device = "cuda")
gpu.matrix(nrow = 2,ncol = 4,dtype = "float632", device = "cuda")
gpu.matrix(nrow = 2,ncol = 4,dtype = "float32", device = "cuda")
torch_full(c(2,3))
device_torch
torch_full(c(2,3),fill_value = torch_float64("NAN"))
torch_full(c(2,3),fill_value = torch_float64(NAN))
torch_full(c(2,3),fill_value = torch_float64(NA))
torch_float64()
torch_full(c(2,3),fill_value = torch_float64(NA))
torch_full(c(2,3),fill_value = torch_float64())
torch_full(c(2,3),fill_value = NULL)
torch_full(c(2,3),fill_value = 2)
torch_full(c(2,3),fill_value = NA)
torch_full(c(2,3),fill_value = F)
torch_full(c(2,3),fill_value = T)
torch_full(c(2,3),fill_value = "NA")
torch_full(c(2,3),fill_value =6 )
torch_full(c(2,3),fill_value =NaN )
matrix(c(T,F,T,F),2,2)
class(c(T,F,T,F))
class(c(T,F,T,F))[1]
class(NaN)
class(NA)
class(NULL)
class(data)[1]
data <- NULL
classData <- class(data)[1]
classData
torch_tensor(c(T,F,T,F))
torch_tensor(c(T,F,T,F),dtype = torch_bool())
torch_tensor(c(T,F,T,F),c(2,2),dtype = torch_bool())
torch_tensor(c(T,F,T,F),2,2,dtype = torch_bool())
data
length(data)
library(tensorflow)
tf <- use_condaenv("tf")
tf
# Efficient implementation
# Define the logistic function
sigmoid <- function(x) {
#.5 * (tanh(.5*x)+1)
# or
1/(1+exp(-x))
}
train_cglr <- function(X,y,beta = NULL, lambda = 0, iterations = 10, tol = 1e-6) {
tX <- t(X)
if (is.null(beta))
beta <- solve(crossprod(X),crossprod(X,2*y-1)) # Returns double even with float inputs. Fix!
p <- sigmoid(X %*% beta)
a <- p*(1-p)
# g <- gr(beta,X,y)
g <- tX %*% (p - y)
u <- g
u_old <- u
for (iter in 1:iterations) {
if (iter == 1) {
uhu <- (sum(u*(tX %*% (a * (X %*% u)))) + lambda * sum(u*u))
beta <- beta-sum(g*u)/uhu * u
} else{
p <- sigmoid(X %*% beta)
beta_old <- beta
g_old <- g
a <- p*(1-p)
g <- tX %*% (p - y)
# g <- t(crossprod(p-y,X)) # Avoid the transposition of a sparse matrix. TODO: remove warnings
k <- g - g_old
beta_coef <- sum(g * k)/sum(u_old*k) # beta Hestenes-Stiefel. Other options out there
u <- g - u_old * beta_coef
u_old <- u
uhu <- sum((X %*% u)^2*a) + lambda * sum(u*u)
beta <- beta_old - sum(g*u)/uhu * u
if(sum(g*g)< tol)
break
}
}
print(iter)
return(beta)
}
library(RcppNumerical)
library(GPUmatrix)
library(torch)
library(tensorflow)
library(profvis)
library(Matrix)
# Example usage:
set.seed(1234)
n <- 30000
p <- 300
X <- cbind(1, matrix(rnorm(n * p), n, p))*.1
beta_true <- rnorm(p+1)
linear_preds_true <- X %*% beta_true
y <- rbinom(n, 1, sigmoid(linear_preds_true))
Xgpu <- gpu.matrix(X, dtype = "float64", device = "cuda")
ygpu <- gpu.matrix(y, dtype = "float64", device = "cuda")
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float64",device="cuda")
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
# system.time(salida <- (train_cglr(X,y, iterations = 1000)))
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than fastLR
#Float32 is slightly faster
Xgpu <- gpu.matrix(X, dtype = "float32", device = "cuda")
ygpu <- gpu.matrix(y, dtype = "float32", device = "cuda")
betainit <- gpu.matrix(betainit, dtype = "float32",device="cuda")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cpu", sparse = F)
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cpu")
betainit <- gpu.matrix(betainit, dtype = "float64", device = "cpu")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000)))
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cpu", sparse = F)
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cpu")
betainit <- gpu.matrix(betainit, dtype = "float64", device = "cpu")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000)))
Xgpu <-     gpu.matrix(X, dtype = "float64", device = "cpu", sparse = F)
ygpu <-     gpu.matrix(y, dtype = "float64", device = "cpu")
betainit <- gpu.matrix(betainit, dtype = "float64", device = "cpu")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000)))
library(Matrix)
Xgpu <-     gpu.matrix(X, dtype = "float32", device = "cpu", sparse = F)
ygpu <-     gpu.matrix(y, dtype = "float32", device = "cpu")
betainit <- gpu.matrix(betainit, dtype = "float32", device = "cpu")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000)))
#Float32 is slightly faster
Xgpu <- gpu.matrix(X, dtype = "float32", device = "cuda")
ygpu <- gpu.matrix(y, dtype = "float32", device = "cuda")
betainit <- gpu.matrix(betainit, dtype = "float32",device="cuda")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
#Float32 is slightly faster
Xgpu <- gpu.matrix(X, dtype = "float32", device = "cuda")
ygpu <- gpu.matrix(y, dtype = "float32", device = "cuda")
betainit <- gpu.matrix(betainit, dtype = "float32",device="cuda")
system.time(salida <- (train_cglr(Xgpu,ygpu, betainit,iterations = 1000))) # Slightly faster than float64
#Float32 is slightly faster
Xgpu <- gpu.matrix(X, dtype = "float32", device = "cuda")
ygpu <- gpu.matrix(y, dtype = "float32", device = "cuda")
betainit <- gpu.matrix(betainit, dtype = "float32",device="cuda")
X<-Xgpu
y<-ygpu
beta <- betainit
tX <- t(X)
GPUmatrix::device(tX)
GPUmatrix:::device(tX)
tX <- t(X)
is.null(beta)
p <- sigmoid(X %*% beta)
p
beta
betainit <- gpu.matrix(betainit, dtype = "float32",device="cuda")
betainit
#Float32 is slightly faster
Xgpu <- gpu.matrix(X, dtype = "float32", device = "cuda")
Xgpu
betainit <- gpu.matrix(runif(ncol(Xgpu)), dtype = "float64",device="cuda")
betainit
betainit <- gpu.matrix(betainit, dtype = "float32",device="cuda")
betainit
